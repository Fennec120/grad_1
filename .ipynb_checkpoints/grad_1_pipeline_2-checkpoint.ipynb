{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb2aa0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer, make_column_selector\n",
    "import time\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import json\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b8766df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def event_action(df3):\n",
    "    print('event_action start')\n",
    "    target_action = ['sub_car_claim_click', \n",
    "                 'sub_car_claim_submit_click',\n",
    "                 'sub_open_dialog_click', \n",
    "                 'sub_custom_question_submit_click', \n",
    "                 'sub_call_number_click', \n",
    "                 'sub_callback_submit_click', \n",
    "                 'sub_submit_success', \n",
    "                 'sub_car_request_submit_click'\n",
    "                ]\n",
    "\n",
    "    df3['event_action'] = df3['event_action'].apply(lambda x: 1 if x in target_action else 0)\n",
    "    \n",
    "    print( 'event_action end')\n",
    "    print('-')      \n",
    "    #print('-')      \n",
    "    #print('-') \n",
    "    \n",
    "    return df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6fce1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_df3(df3, total_rows =  200000, neg_percent = 50, pos_percent = 50):\n",
    "    print( 'sample_df3 start')\n",
    "    df3_pos = df3[df3['event_action'] == 1].sample(int(total_rows / 100 * pos_percent))\n",
    "    df3_neg = df3[df3['event_action'] == 0].sample(int(total_rows / 100 * neg_percent))\n",
    "    df3_pos = df3_pos.reset_index()\n",
    "    df3_neg = df3_neg.reset_index()\n",
    "    df3_pos = df3_pos.drop('index', axis=1)\n",
    "    df3_neg = df3_neg.drop('index', axis=1)\n",
    "    df3 = pd.concat([df3_pos, df3_neg])\n",
    "    \n",
    "    print( 'sample_df3 end')\n",
    "    print('-')      \n",
    "    #print('-')      \n",
    "    #print('-') \n",
    "    \n",
    "    return df3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5dcd9d",
   "metadata": {},
   "source": [
    "df = pd.read_csv('data/ga_hits.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4bb9ea",
   "metadata": {},
   "source": [
    "df2 = pd.read_csv('data/ga_sessions.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e5c567",
   "metadata": {},
   "source": [
    "df3 = pd.merge(df, df2, on='session_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5cc29ae",
   "metadata": {},
   "source": [
    "df3 = event_action(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "540c97a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ad_campaign(df3):\n",
    "    print( 'ad_campaign start')\n",
    "    try:\n",
    "        with open('data/utm_c_frec_dict2.json', 'r') as f:\n",
    "            utm_c_frec_dict = json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        print(\"oh.., looks like its the first time you run it - lil' bit longer then, m8. pls hold:)\")\n",
    "\n",
    "        utm_c_frec_dict = {}\n",
    "        counter = 1\n",
    "        for pos in df3.utm_campaign.unique():\n",
    "            if len(df3[(df3.utm_campaign == pos) & (df3.event_action == 1)]) == 0:\n",
    "                utm_c_frec_dict[str(pos)] = 0\n",
    "            else:\n",
    "                utm_c_frec_dict[str(pos)] = round(len(df3[(df3.utm_campaign == pos) & (df3.event_action == 1)]) / len(df3[df3.utm_campaign == pos]), 5)\n",
    "\n",
    "            #print(counter)\n",
    "            counter = counter  + 1\n",
    "        with open('data/utm_c_frec_dict2.json', 'w') as f: \n",
    "            json.dump(utm_c_frec_dict, f)\n",
    "\n",
    "    finally:\n",
    "        df3['camp_succ_rate'] = df3.utm_campaign.apply(lambda x: utm_c_frec_dict[str(x)])\n",
    "    \n",
    "    #print(utm_c_frec_dict)\n",
    "    print( 'ad_campaign end')\n",
    "    print('-')      \n",
    "    #print('-')      \n",
    "    #print('-') \n",
    "    \n",
    "    return df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ed08b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ad_campaign_v_2(df3):\n",
    "    \n",
    "    print( 'ad_campaign v2 start')\n",
    "    \n",
    "    \n",
    "    \n",
    "    try:\n",
    "        with open('data/utm_c_frec_dict3.json', 'r') as f:\n",
    "            utm_c_frec_dict = json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        print(\"oh.., looks like its the first time you run it - lil' bit longer then, m8. pls hold:)\")\n",
    "        df5 = df3[['utm_campaign', 'event_action']]\n",
    "        succ_camps = df5[df5.event_action == 1].utm_campaign.value_counts(dropna=False)\n",
    "        all_camps = df5.utm_campaign.value_counts(dropna=False)\n",
    "\n",
    "        utm_c_frec_dict = {}\n",
    "        \n",
    "        for pos in all_camps.keys():\n",
    "            if pos in succ_camps.keys():\n",
    "                utm_c_frec_dict[str(pos)] = round(succ_camps[pos] / all_camps[pos], 5)\n",
    "            else:\n",
    "                utm_c_frec_dict[str(pos)] = 0\n",
    "        \n",
    "        \n",
    "        with open('data/utm_c_frec_dict3.json', 'w') as f: \n",
    "            json.dump(utm_c_frec_dict, f)\n",
    "\n",
    "    finally:\n",
    "        df3['camp_succ_rate'] = df3.utm_campaign.apply(lambda x: utm_c_frec_dict[str(x)])\n",
    "    \n",
    "    #print(utm_c_frec_dict)\n",
    "    print( 'ad_campaign v2 end')\n",
    "    print('-')      \n",
    "    #print('-')      \n",
    "    #print('-') \n",
    "    \n",
    "    return df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40072e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def day_of_week(df3):\n",
    "    print( 'day_of_week start')\n",
    "    df3['new_date'] = pd.to_datetime(df3['visit_date'])\n",
    "    df3['day_of_week'] = df3.new_date.dt.dayofweek\n",
    "    \n",
    "    df3 = df3.drop('new_date', axis=1)\n",
    "    print('day_of_week end')\n",
    "    print('-')      \n",
    "    #print('-')      \n",
    "    #print('-') \n",
    "    \n",
    "    return df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21610a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def empties(df3):\n",
    "    print( 'empties end')\n",
    "    df3.loc[df3.utm_source.isna() == True, 'utm_source'] = 'other'\n",
    "    df3.loc[df3.utm_adcontent.isna() == True, 'utm_adcontent'] = 'Other'\n",
    "    df3.loc[df3.device_brand.isna() == True, 'device_brand'] = 'other'\n",
    "    \n",
    "    print( 'empties end')\n",
    "    print('-')      \n",
    "    #print('-')      \n",
    "    #print('-')  \n",
    "    \n",
    "    return df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41d98c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resolution_func(df3):\n",
    "    print('resolution_func start')\n",
    "    #resolution\n",
    "    bounds = []\n",
    "    df3['resolution'] = df3.device_screen_resolution.apply(lambda x:eval(x.replace('x','*')))\n",
    "    for device in df3.device_category.unique():\n",
    "        q25 = df3[df3.device_category == device].resolution.quantile(0.25)\n",
    "        q75 = df3[df3.device_category == device].resolution.quantile(0.75)\n",
    "        iqr = q75 - q25\n",
    "        bounds.append((device, q25 - 1.5 * iqr, q75 + 1.5 * iqr))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    test_list = list(df3.device_screen_resolution)\n",
    "    test_list2 = list(df3.device_category)\n",
    "\n",
    "    for i in range(len(test_list)):\n",
    "        test_list[i] = eval(test_list[i].replace('x','*'))\n",
    "\n",
    "    tst_l = list(zip(test_list2, test_list))\n",
    "\n",
    "    resolution = []\n",
    "\n",
    "    for i in range(len(tst_l)):\n",
    "        if tst_l[i][0] == bounds[0][0]:\n",
    "            resolution.append(bounds[0][0]+'_high' if tst_l[i][1] >= bounds[0][2] * 0.7 else (bounds[0][0]+'_medium' if bounds[0][2] * 0.7 > tst_l[i][1] >= bounds[0][2] * 0.1 else bounds[0][0]+'_low'))\n",
    "        elif tst_l[i][0] == bounds[1][0]:\n",
    "            resolution.append(bounds[1][0]+'_high' if tst_l[i][1] >= bounds[1][2] * 0.7 else (bounds[1][0]+'_medium' if bounds[1][2] * 0.7 > tst_l[i][1] >= bounds[1][2] * 0.1 else bounds[1][0]+'_low'))\n",
    "        elif tst_l[i][0] == bounds[2][0]:\n",
    "            resolution.append(bounds[2][0]+'_high' if tst_l[i][1] >= bounds[2][2] * 0.7 else (bounds[2][0]+'_medium' if bounds[2][2] * 0.7 > tst_l[i][1] >= bounds[2][2] * 0.1 else bounds[2][0]+'_low'))\n",
    "\n",
    "    df3['device_screen_resolution_engeneered'] = resolution\n",
    "    #df3['device_screen_resolution'] = resolution\n",
    "    df3 = df3.drop('resolution', axis=1)\n",
    "    \n",
    "    print('resolution_func end')\n",
    "    print('-')      \n",
    "    #print('-')      \n",
    "    #print('-') \n",
    "    \n",
    "    \n",
    "    return df3\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f81310c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resolution_func_v_2(df3):\n",
    "    print('resolution_func v2 start')\n",
    "    #resolution\n",
    "    bounds = []\n",
    "    df3['device_screen_resolution'] = df3.device_screen_resolution.apply(lambda x:eval(x.replace('x','*')))\n",
    "\n",
    "    \n",
    "    print('resolution_func v2 end')\n",
    "    print('-')      \n",
    "    #print('-')      \n",
    "    #print('-') \n",
    "    \n",
    "    \n",
    "    return df3\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "23b6e2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def country(df3, trsh = 0.001):\n",
    "    print('country start')\n",
    "    #geo_country\n",
    "    country_list = list(df3.geo_country.unique())\n",
    "    for i in range(len(country_list)):\n",
    "        country_list[i] = ( len(df3[df3.geo_country == country_list[i]]), country_list[i])\n",
    "    country_list = sorted(country_list, reverse=True)\n",
    "\n",
    "#    trsh = 0.0005\n",
    "    df3_len = len(df3) \n",
    "    for item in country_list:\n",
    "        if item[0] / df3_len >= trsh:\n",
    "            continue\n",
    "        else:\n",
    "            df3.loc[df3.geo_country == item[1], 'geo_country'] = 'some_unimportant_country'\n",
    "    \n",
    "    print( 'country end')\n",
    "    print('-')      \n",
    "    #print('-')      \n",
    "    #print('-') \n",
    "    \n",
    "    return df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3af77a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def country_v_2(df3):\n",
    "    print('country v2  start')\n",
    "    #geo_country\n",
    "    counter = 0\n",
    "    country_list_new = dict()\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        with open('data/country_list_new.txt', 'r') as f:\n",
    "            for line in f:\n",
    "                # remove newline character and parentheses\n",
    "                line = line.rstrip('\\n').replace(\"%\", '')\n",
    "                tuple_elements = line.split('*')\n",
    "                my_tuple = (tuple_elements[0], eval(tuple_elements[1]))\n",
    "                country_list_new[my_tuple[0]] = my_tuple[1]\n",
    "\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(\"oh.., looks like its the first time you run it - lil' bit longer then, m8. pls hold:)\")\n",
    "        \n",
    "        succ_total = len(df3[df3.event_action == 1])\n",
    "        country_list_success = df3[df3.event_action == 1].geo_country.value_counts().sort_values(ascending=False)\n",
    "        country_list_new = []\n",
    "        for country in country_list_success.keys():\n",
    "            country_list_new.append(f'{country}*{str(round(country_list_success[country] / succ_total, 4))}%')\n",
    "            counter += 1\n",
    "            if counter == 23:\n",
    "                break\n",
    "\n",
    "        with open('data/country_list_new.txt', 'w') as f:\n",
    "            for t in country_list_new:\n",
    "                f.write(str(t) +'\\n')\n",
    "                \n",
    "        country_list_new = dict()        \n",
    "        with open('data/country_list_new.txt', 'r') as f:\n",
    "            for line in f:\n",
    "                # remove newline character and parentheses\n",
    "                line = line.rstrip('\\n').replace(\"%\", '')\n",
    "                tuple_elements = line.split('*')\n",
    "                my_tuple = (tuple_elements[0], eval(tuple_elements[1]))\n",
    "                country_list_new[my_tuple[0]] = my_tuple[1]                \n",
    "\n",
    "\n",
    "    finally:\n",
    "        df3['geo_country_succ_perc'] = df3['geo_country'].apply(lambda x: country_list_new[x] if x in country_list_new else 0.0001)\n",
    "        \n",
    "    \n",
    "\n",
    "    #print(sum(df4.isnull().sum().values))\n",
    "    #print(df4.isnull().sum())\n",
    "    print('country v2 end')    \n",
    "    print('-')      \n",
    "    #print('-')      \n",
    "    #print('-')     \n",
    "    \n",
    "    return df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "82a5c836",
   "metadata": {},
   "outputs": [],
   "source": [
    "def city(df3, trsh = 0.001):\n",
    "    print('city start')\n",
    "    #geo_city\n",
    "    city_list = []\n",
    "    df3_len = len(df3)\n",
    "    try:\n",
    "        with open('data/city_list1.txt', 'r') as f:\n",
    "            for line in f:\n",
    "                # remove newline character and parentheses\n",
    "                line = line.rstrip('\\n').replace('(', '').replace(')', '').replace(\"'\", '')\n",
    "                # split on comma and convert each element to correct type\n",
    "                tuple_elements = [int(e.strip()) if e.strip().isdigit() else e.strip() for e in line.split(',')]\n",
    "                # create tuple and add to list\n",
    "                my_tuple = tuple(tuple_elements)\n",
    "                city_list.append(my_tuple)\n",
    "\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(\"oh.., looks like its the first time you run it - lil' bit longer then, m8. pls hold:)\")\n",
    "\n",
    "        city_list = list(zip(df3.geo_city.value_counts().values, df3.geo_city.value_counts().keys() ))\n",
    "        city_list = sorted(city_list, reverse=True)\n",
    "\n",
    "        with open('data/city_list1.txt', 'w') as f:\n",
    "            for t in city_list:\n",
    "                f.write(str(t) +'\\n')\n",
    "\n",
    "\n",
    "\n",
    "    finally:\n",
    "#        trsh = 0.0005\n",
    "        city_list_valid = []\n",
    "        \n",
    "        for item in city_list:\n",
    "            #print(item[1], ' - ', round(item[0] / df3_len, 4),'%' )\n",
    "            if round(item[0] / 15000000, 4) >= trsh:   #df3_len, 4) >= trsh:\n",
    "                city_list_valid.append(item[1])\n",
    "                #print('trsh == 2000 - ', item[0], item[1], round(item[0] / df3_len, 4) >= trsh, ' - appended')\n",
    "\n",
    "        df3.loc[(~df3['geo_city'].isin(city_list_valid)), 'geo_city'] = 'some_unimportant_city'\n",
    "    \n",
    "    \n",
    "\n",
    "    #print(sum(df4.isnull().sum().values))\n",
    "    #print(df4.isnull().sum())\n",
    "    print('city end')    \n",
    "    print('-')      \n",
    "    #print('-')      \n",
    "    #print('-')      \n",
    "    \n",
    "    return df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d07109ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def city_v_2(df3):\n",
    "    print('city v2  start')\n",
    "    #geo_city\n",
    "    counter = 0\n",
    "    city_list_new = dict()\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        with open('data/city_list_new.txt', 'r') as f:\n",
    "            for line in f:\n",
    "                # remove newline character and parentheses\n",
    "                line = line.rstrip('\\n').replace(\"%\", '')\n",
    "                tuple_elements = line.split('*')\n",
    "                my_tuple = (tuple_elements[0], eval(tuple_elements[1]))\n",
    "                city_list_new[my_tuple[0]] = my_tuple[1]\n",
    "\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(\"oh.., looks like its the first time you run it - lil' bit longer then, m8. pls hold:)\")\n",
    "        \n",
    "        succ_total = len(df3[df3.event_action == 1])\n",
    "        city_list_success = df3[df3.event_action == 1].geo_city.value_counts().sort_values(ascending=False)\n",
    "        city_list_new = []\n",
    "        for city in city_list_success.keys():\n",
    "            city_list_new.append(f'{city}*{str(round(city_list_success[city] / succ_total, 4))}%')\n",
    "            counter += 1\n",
    "            if counter == 26:\n",
    "                break\n",
    "\n",
    "        with open('data/city_list_new.txt', 'w') as f:\n",
    "            for t in city_list_new:\n",
    "                f.write(str(t) +'\\n')\n",
    "                \n",
    "        with open('data/city_list_new.txt', 'r') as f:\n",
    "            city_list_new = dict()\n",
    "            for line in f:\n",
    "                # remove newline character and parentheses\n",
    "                line = line.rstrip('\\n').replace(\"%\", '')\n",
    "                tuple_elements = line.split('*')\n",
    "                my_tuple = (tuple_elements[0], eval(tuple_elements[1]))\n",
    "                city_list_new[my_tuple[0]] = my_tuple[1]                \n",
    "\n",
    "\n",
    "    finally:\n",
    "        \n",
    "        df3['geo_city_succ_perc'] = df3['geo_city'].apply(lambda x: city_list_new[x] if x in city_list_new else 0.0001)\n",
    "        \n",
    "    \n",
    "\n",
    "    #print(sum(df4.isnull().sum().values))\n",
    "    #print(df4.isnull().sum())\n",
    "    print('city v2 end')    \n",
    "    print('-')      \n",
    "    #print('-')      \n",
    "    #print('-')      \n",
    "    \n",
    "    return df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ae66ea27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def device_brand_v_2(df3):\n",
    "    print('device_brand v2 start')\n",
    "    #device_brand\n",
    "    counter = 0\n",
    "    device_brand_list_new = dict()\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        with open('data/device_brand_list_new1.txt', 'r') as f:\n",
    "            for line in f:\n",
    "                # remove newline character and parentheses\n",
    "                line = line.rstrip('\\n').replace(\"%\", '')\n",
    "                tuple_elements = line.split('*')\n",
    "                my_tuple = (tuple_elements[0], eval(tuple_elements[1]))\n",
    "                device_brand_list_new[my_tuple[0]] = my_tuple[1]\n",
    "\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(\"oh.., looks like its the first time you run it - lil' bit longer then, m8. pls hold:)\")\n",
    "        succ_total = len(df3[df3.event_action == 1])\n",
    "        device_brand_list_success = df3[df3.event_action == 1].device_brand.value_counts().sort_values(ascending=False)\n",
    "        device_brand_list_new = []\n",
    "        for device_brand in device_brand_list_success.keys():\n",
    "            device_brand_list_new.append(f'{device_brand}*{str(round(device_brand_list_success[device_brand] / succ_total, 4))}%')\n",
    "            counter += 1\n",
    "            if counter == 23:\n",
    "                break\n",
    "\n",
    "        with open('data/device_brand_list_new1.txt', 'w') as f:\n",
    "            for t in device_brand_list_new:\n",
    "                f.write(str(t) +'\\n')\n",
    "                \n",
    "        device_brand_list_new = dict()\n",
    "        with open('data/device_brand_list_new1.txt', 'r') as f:\n",
    "            for line in f:\n",
    "                # remove newline character and parentheses\n",
    "                line = line.rstrip('\\n').replace(\"%\", '')\n",
    "                tuple_elements = line.split('*')\n",
    "                my_tuple = (tuple_elements[0], eval(tuple_elements[1]))\n",
    "                device_brand_list_new[my_tuple[0]] = my_tuple[1]                \n",
    "\n",
    "\n",
    "    finally:\n",
    "        df3['device_brand_succ_perc'] = df3['device_brand'].apply(lambda x: device_brand_list_new[x] if x in device_brand_list_new else 0.0001)\n",
    "        \n",
    "    \n",
    "\n",
    "    #print(sum(df4.isnull().sum().values))\n",
    "    #print(df4.isnull().sum())\n",
    "    print('device_brand v2 end')    \n",
    "    print('-')      \n",
    "    #print('-')      \n",
    "    #print('-')      \n",
    "    \n",
    "    return df3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e81911",
   "metadata": {},
   "source": [
    "df3 = device_brand_v_2(df3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee55eddd",
   "metadata": {},
   "source": [
    "df4 = pd.read_csv('data/df3_10k_50n_50p.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301b1508",
   "metadata": {},
   "source": [
    "df4 = device_brand_v_2(df4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893ca5c5",
   "metadata": {},
   "source": [
    "df4.device_brand.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c2719895",
   "metadata": {},
   "outputs": [],
   "source": [
    "def device_brand(df3, trsh = 0.0012):\n",
    "    print('device_brand start')\n",
    "    #device_brand\n",
    "    brand_list = []\n",
    "    df3_len = len(df3)\n",
    "    \n",
    "    try:\n",
    "        with open('data/brand_list1.txt', 'r') as f:\n",
    "            for line in f:\n",
    "                # remove newline character and parentheses\n",
    "                line = line.rstrip('\\n').replace('(', '').replace(')', '').replace(\"'\", '')\n",
    "                # split on comma and convert each element to correct type\n",
    "                tuple_elements = [int(e.strip()) if e.strip().isdigit() else e.strip() for e in line.split(',')]\n",
    "                # create tuple and add to list\n",
    "                my_tuple = tuple(tuple_elements)\n",
    "                brand_list.append(my_tuple)\n",
    "\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(\"oh.., looks like its the first time you run it - lil' bit longer then, m8. pls hold:)\")\n",
    "\n",
    "        brand_list = list(zip(df3.device_brand.value_counts().values, df3.device_brand.value_counts().keys() ))\n",
    "        brand_list = sorted(brand_list, reverse=True)\n",
    "\n",
    "        with open('data/brand_list1.txt', 'w') as f:\n",
    "            for t in brand_list:\n",
    "                f.write(str(t) +'\\n')\n",
    "\n",
    "\n",
    "\n",
    "    finally:\n",
    "#        trsh = 0.0005\n",
    "        brand_list_valid = []\n",
    "        \n",
    "        for item in brand_list:\n",
    "            #print(item[0], ' ', item[0] / df3_len,'>=', trsh, ' ', round(item[0] / df3_len, 4) >= trsh )\n",
    "            if item[0] / df3_len >= trsh:\n",
    "                brand_list_valid.append(item[1])\n",
    "                #print(len(brand_list_valid), ' ', item[0],' ',item[1] )\n",
    "\n",
    "        df3.loc[(~df3['device_brand'].isin(brand_list_valid)), 'device_brand'] = 'some_unimportant_brand'\n",
    "    \n",
    "    \n",
    "    print('device_brand end')    \n",
    "    print('-')      \n",
    "    #print('-')      \n",
    "    #print('-')      \n",
    "    \n",
    "    return df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ff60f6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_stuff(df3):\n",
    "    print('encode_stuff start')\n",
    "    cols_to_encode = ['utm_source', \n",
    "                      'utm_medium', \n",
    "                      'utm_adcontent', \n",
    "                      #'device_brand', \n",
    "                      'device_category', \n",
    "                      'device_screen_resolution', \n",
    "                      'device_browser',\n",
    "                      'utm_campaign'\n",
    "                      #,'geo_country',\n",
    "                      #'geo_city'\n",
    "                     ]\n",
    "    \n",
    "    #encoding\n",
    "    encoded_features = pd.DataFrame()\n",
    "\n",
    "    for col in cols_to_encode:\n",
    "\n",
    "        pre_encoded_df3 = df3[[col]]\n",
    "        encoder = OneHotEncoder(categories='auto', handle_unknown='ignore', sparse=False)\n",
    "        encoded_array = encoder.fit_transform(pre_encoded_df3)\n",
    "        #feature_names = [f'{col}_{name}' for name in encoder.get_feature_names_out()]\n",
    "        feature_names = encoder.get_feature_names_out()\n",
    "        encoded_df3 = pd.DataFrame(encoded_array, columns=feature_names)\n",
    "\n",
    "        #if len(encoded_features) == 0:\n",
    "        #    encoded_features = encoded_df3.copy()\n",
    "        #else:\n",
    "        #    encoded_features[feature_names] = encoded_df3.values\n",
    "        \n",
    "        df3[feature_names] = encoded_df3.values\n",
    "    #print(encoded_features.isnull().sum())\n",
    "\n",
    "    #df3 = df3.join(encoded_features)\n",
    "    #print(df3.isnull().sum())\n",
    "    df3 = df3.drop(cols_to_encode, axis=1)\n",
    "    print( 'encode_stuff end')\n",
    "    print('-')      \n",
    "    #print('-')      \n",
    "    #print('-')  \n",
    "    \n",
    "    return df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "439622ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_stuff(df3):\n",
    "    #scaling\n",
    "    print('scale_stuff start')\n",
    "    cols_to_scale = [#'visit_number',\n",
    "                     'day_of_week',\n",
    "                     'device_screen_resolution'\n",
    "                    ]\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    scaled_features = scaler.fit_transform(df3.loc[:,cols_to_scale])\n",
    "    scaled_feature_names = [f'{name}_scaled' for name in scaler.get_feature_names_out()]\n",
    "    #scaler.get_feature_names_out()\n",
    "\n",
    "    #scaled_df = pd.DataFrame(scaled_features, columns=scaled_feature_names)\n",
    "    df3[scaled_feature_names] = scaled_features\n",
    "    #print(scaled_df.shape, scaled_df.columns)\n",
    "    #print(scaled_df.isnull().sum())\n",
    "\n",
    "    #df3['scaled_feature_names'] = scaled_df\n",
    "    #print(df3.shape, df3.columns)\n",
    "    df3 = df3.drop(cols_to_scale, axis=1)\n",
    "    for column in df3.columns:\n",
    "        print(column)\n",
    "    print(len(df3.columns))\n",
    "    #print(df3.isnull().sum())\n",
    "    #print(len(df3.columns), df3.columns)\n",
    "    print('scale_stuff end')\n",
    "    print('-')      \n",
    "    #print('-')      \n",
    "    #print('-')  \n",
    "    \n",
    "    return df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f2a7612e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_stuff(df3):\n",
    "    #pre-existing list of columns\n",
    "    print('filter_stuff start')\n",
    "    cols_to_drop = [\n",
    "        'session_id',\n",
    "        'hit_date',\n",
    "        'hit_time',\n",
    "        'hit_number',\n",
    "        'hit_type',\n",
    "        'hit_referer',\n",
    "        'hit_page_path',\n",
    "        'event_category',\n",
    "        'event_label',\n",
    "        'event_value',\n",
    "        'client_id',\n",
    "        #'new_date',\n",
    "        'visit_date',\n",
    "        'visit_number',\n",
    "        'utm_keyword',\n",
    "        'device_os',\n",
    "        'device_model',\n",
    "        'visit_time'\n",
    "    ]\n",
    "    \n",
    "    cols_to_encode = [\n",
    "        'utm_source',\n",
    "        'utm_medium', \n",
    "        'utm_adcontent',\n",
    "        'device_brand', \n",
    "        'device_category', \n",
    "        'device_screen_resolution',\n",
    "        'device_browser',\n",
    "        'utm_campaign',\n",
    "        'geo_country',\n",
    "        'geo_city'\n",
    "    ]\n",
    "    #dropping\n",
    "    #cols_to_drop = []\n",
    "    #for col in df_columns:\n",
    "    #    cols_to_drop.append(str(col))\n",
    "    #cols_to_drop = cols_to_drop + ['client_id','new_date', 'visit_date', 'utm_keyword', 'device_os', 'device_model', 'visit_time']    \n",
    "    \n",
    "    df3 = df3.drop(cols_to_drop, axis=1)\n",
    "    #df3 = df3.drop(cols_to_encode, axis=1)\n",
    "    \n",
    "    try:\n",
    "        df3 = df3.drop('Unnamed: 0', axis=1)\n",
    "    except KeyError:\n",
    "        pass\n",
    "    try:\n",
    "        df3 = df3.drop('Unnamed: 0.1', axis=1)\n",
    "    except KeyError:\n",
    "        pass\n",
    "    try:\n",
    "        df3 = df3.drop('Unnamed: 0.2', axis=1)\n",
    "    except KeyError:\n",
    "        pass\n",
    "    \n",
    "    print('filter_stuff end')\n",
    "    #print(sum(df3.isnull().sum().values))\n",
    "    #print(df3.isnull().sum())\n",
    "    print(df3.columns)\n",
    "    print('-')      \n",
    "    #print('-')      \n",
    "    #print('-') \n",
    "    \n",
    "    return df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "859940a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_stuff(df3):\n",
    "    #checking\n",
    "    print('check_stuff start')\n",
    "    counter = 0\n",
    "    for feature in df3.columns:\n",
    "        if df3[feature].dtype != 'O':\n",
    "            #print(feature, ' - ', df3[feature].dtype)\n",
    "            counter += 1\n",
    "        else:\n",
    "            print(feature)\n",
    "    print(counter == len(df3.columns))\n",
    "\n",
    "\n",
    "    #checking 2\n",
    "    counter = 0\n",
    "    for feature in df3.columns:\n",
    "        if len(df3[df3[str(feature)].isna() == True]) != 0:\n",
    "            print(feature, ' - ', len(df3[df3[str(feature)].isna() == True]))\n",
    "            counter += 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    if counter == 0:\n",
    "        print('vse zaebis\", pustukh fi4ei net')    \n",
    "    \n",
    "    \n",
    "    print('check_stuff end')\n",
    "    print('-')      \n",
    "    #print('-')      \n",
    "    #print('-') \n",
    "    \n",
    "    return df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "814b693a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_stuff_2(df3):\n",
    "    #checking\n",
    "    print('check_stuff_2 start')\n",
    "\n",
    "    counter = 0\n",
    "    for feature in df3.columns:\n",
    "        if df3[feature].dtype != 'O':\n",
    "            #print(feature, ' - ', df3[feature].dtype)\n",
    "            counter += 1\n",
    "        else:\n",
    "            print(feature)\n",
    "    print(counter == len(df3.columns))\n",
    "\n",
    "\n",
    "    #checking 2\n",
    "    empty_features = False\n",
    "    if sum(df3.isnull().sum()) != 0:\n",
    "        print(df3.isnull().sum())\n",
    "        empty_features = True\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    if empty_features == False:\n",
    "        print('vse zaebis\", pustukh fi4ei net') \n",
    "    #print(len(df3.isnull().sum()))\n",
    "    print(df3.shape,  'check_stuff_2 end') #df3.shape,\n",
    "    print('-')      \n",
    "    #print('-')      \n",
    "    #print('-')     \n",
    "    \n",
    "    \n",
    "    return df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b018e30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_stuff_3(df3):\n",
    "    for column in df3.columns:\n",
    "        print(column)\n",
    "        print(df3[column].value_counts())\n",
    "    print(len(df3.columns))\n",
    "    print(' - ')\n",
    "    \n",
    "    return df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a48f726b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_stuff(df3):\n",
    "    y = df3['event_action']\n",
    "    \n",
    "    df3 = df3.drop('event_action', axis=1)\n",
    "    print(df3.columns)\n",
    "    x_train, x_test, y_train, y_test = train_test_split(df3,y, test_size=0.3)\n",
    "    \n",
    "    rf = RandomForestClassifier(n_estimators=400, min_samples_leaf=2, max_features='sqrt')\n",
    "    rf.fit(x_train, y_train)\n",
    "    \n",
    "    predicted_train = rf.predict(x_train)\n",
    "    predicted_test = rf.predict(x_test)\n",
    "    \n",
    "    #print(df3.shape, ' - shape', ' function - ')\n",
    "    \n",
    "    \n",
    "    print('train acc score - ',accuracy_score(y_train, predicted_train))\n",
    "    print('test acc score - ', accuracy_score(y_test, predicted_test))\n",
    "\n",
    "    print('train roc score - ',roc_auc_score(y_train, rf.predict_proba(x_train)[:,1]))\n",
    "    print('test roc score - ',roc_auc_score(y_test, rf.predict_proba(x_test)[:,1]))\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c73138c",
   "metadata": {},
   "source": [
    "with open('models/rf_model.pkl', 'wb') as file:\n",
    "    dill.dump(rf, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d65451",
   "metadata": {},
   "source": [
    "## function declarations end here. its wildlands after that...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e72138",
   "metadata": {},
   "source": [
    "df = pd.read_csv('data/ga_hits.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7945579",
   "metadata": {},
   "source": [
    "df2 = pd.read_csv('data/ga_sessions.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe5c01f",
   "metadata": {},
   "source": [
    "df3 = pd.merge(df, df2, on='session_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272d113e",
   "metadata": {},
   "source": [
    "df3 = event_action(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "301c1097",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_scores = [\n",
    "    [0.6603, 'ad_camp V, resol V, cntry V, ct V, brand V, 200k 50/50'],\n",
    "    [0.6576, 'ad_camp V, resol V, cntry V, ct V, brand V, 200k 70/30'],\n",
    "    [0.6456, 'ad_camp V, resol V, cntry V, ct V, brand V, 140k 30/70'],\n",
    "    [0.6469, 'ad_camp V, resol V, cntry V, ct V, brand V, 100k 50/50'],\n",
    "    [0.6428, 'ad_camp V, resol V, cntry V, ct V, brand V, 100k 70/30'],\n",
    "    [0.6447, 'ad_camp V, resol V, cntry V, ct V, brand V, 100k 30/70'],\n",
    "    [0.6438, 'ad_camp v2 V, resol V, cntry V, ct V, brand V, 100k 30/70'],\n",
    "    [0.6445, 'ad_camp v2 V, resol V, cntry V, ct V, brand v2 V, 100k 30/70'],\n",
    "    [0.6459, 'ad_camp V, resol V, cntry V, ct V, brand v2 V, 100k 30/70'],\n",
    "    [0.6312, 'ad_camp V, resol V, cntry V, ct v2 V, brand v2 V, 100k 30/70'],\n",
    "    [0.6463, 'ad_camp V, resol V, cntry v 2V, ct V, brand v2 V, 100k 30/70'],\n",
    "    [0.6461, 'ad_camp V, resol v2 V, cntry v2 V, ct V, brand v2 V, 100k 30/70'],\n",
    "    [0.6482, 'ad_camp V, resol v2 V, cntry v2 V, ct V, brand v2 V, 100k 50/50'],\n",
    "    [0.6486, 'ad_camp V, resol v2 V, cntry v2 V, ct V, brand v2 V, 100k 50/50'],\n",
    "    [0.6463, 'ad_camp X, resol v2 V, cntry v2 V, ct V, brand v2 V, 100k 50/50'],\n",
    "    [0.6489, 'ad_camp V, resol v2 V, cntry v2 V, cntry V, ct v2 V, ct V, brand v2 V, 100k 50/50'],\n",
    "    [0.6605, 'ad_camp V, resol v2 V, cntry v2 V, cntry V, ct v2 V, ct V, brand v2 V, 200k 50/50']\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "46f13be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_single_fit = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444aeaed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = pd.read_csv('data/df3_200k_50n_50p.csv')\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    #('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "\n",
    "f_transformer = ColumnTransformer(transformers=[\n",
    "    ('categorical', categorical_transformer, make_column_selector(dtype_include='object'))\n",
    "])\n",
    "\n",
    "preprocessor = Pipeline(steps=[\n",
    "    #('event', FunctionTransformer(event_action)),\n",
    "    #('sampling', FunctionTransformer(sample_df3)),\n",
    "    ('ad_campaign_feature_creating', FunctionTransformer(ad_campaign)),\n",
    "    ('day_of_week', FunctionTransformer(day_of_week)),\n",
    "    ('empties', FunctionTransformer(empties)),\n",
    "    #('resolution_func', FunctionTransformer(resolution_func)),\n",
    "    ('resolution_func v2', FunctionTransformer(resolution_func_v_2)),\n",
    "    ('country v2', FunctionTransformer(country_v_2)),\n",
    "    ('country', FunctionTransformer(country)),\n",
    "    ('city v2', FunctionTransformer(city_v_2)),\n",
    "    ('city', FunctionTransformer(city)),\n",
    "    ('device brand v2', FunctionTransformer(device_brand_v_2)),\n",
    "    ('device brand', FunctionTransformer(device_brand)),\n",
    "    ('filter_stuff', FunctionTransformer(filter_stuff)),\n",
    "    #('encode_stuff', FunctionTransformer(encode_stuff)),\n",
    "    ('scale_stuff(resol, )', FunctionTransformer(scale_stuff)),\n",
    "    #('check_stuff_3', FunctionTransformer(check_stuff_3)),\n",
    "    ('f_transformer', f_transformer),\n",
    "    #('filter_stuff', FunctionTransformer(filter_stuff)),\n",
    "    #('check_stuff_3', FunctionTransformer(check_stuff_3))\n",
    "])\n",
    "\n",
    "models = [\n",
    "    #RandomForestClassifier(n_estimators=300, max_depth= 10, max_features='sqrt', min_samples_split=2),\n",
    "    #SVC(C=10, gamma=0.01, kernel='rbf'),\n",
    "    #DecisionTreeClassifier(criterion='gini', max_depth=7, min_samples_split=10),\n",
    "    #LogisticRegression( C=1.0, penalty='l2', solver='saga'),\n",
    "    MLPClassifier(hidden_layer_sizes=(100, ), solver='adam', activation='tanh')\n",
    "    ]\n",
    "\n",
    "for model in models:\n",
    "    \n",
    "\n",
    "    pipeline = Pipeline(steps = [\n",
    "        ('preprocessor', preprocessor), \n",
    "        ('classifier', model)  \n",
    "    ])\n",
    "\n",
    "    y = df4['event_action']\n",
    "    x = df4.drop('event_action', axis=1)\n",
    "\n",
    "    #x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.3)\n",
    "    \n",
    "    \n",
    "    #predictions = pipeline.predict(x_test)\n",
    "    #probs = pipeline.predict_proba(x_test)\n",
    "    #scores_single_fit.append([type(model).__name__,' test roc score - ', roc_auc_score(y_test, probs[:,1])])\n",
    "    #scores_single_fit.append([type(model).__name__,' test acc score - ', accuracy_score(y_test, predictions)])\n",
    "    #print(type(model).__name__,' test roc score - ', roc_auc_score(y_test, probs[:,1]))\n",
    "    #print(type(model).__name__,' test acc score - ', accuracy_score(y_test, predictions))\n",
    "    #interm_scores.append((str(model), 'test roc score - ', roc_auc_score(y_test, pipeline.predict_proba(x_test)[:,1])))\n",
    "    #interm_scores.append((str(model), 'test acc score - ', accuracy_score(y_test, predictions)))\n",
    "    \n",
    "    log = 'ad_camp V, resol V, cntry v2 V, cntry V, ct v2 V, ct V, brand v2 V, 200k 50/50'\n",
    "    score = cross_val_score(pipeline, x, y, cv=4, scoring='roc_auc')\n",
    "    #score = cross_val_score(pipeline, x, y, cv=4, scoring='accuracy')\n",
    "    cv_scores.append([ round(score.mean(), 4), log]) #type(model).__name__,\n",
    "    pipeline.fit(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "19f453fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.6603, 'ad_camp V, resol V, cntry V, ct V, brand V, 200k 50/50'],\n",
       " [0.6576, 'ad_camp V, resol V, cntry V, ct V, brand V, 200k 70/30'],\n",
       " [0.6456, 'ad_camp V, resol V, cntry V, ct V, brand V, 140k 30/70'],\n",
       " [0.6469, 'ad_camp V, resol V, cntry V, ct V, brand V, 100k 50/50'],\n",
       " [0.6428, 'ad_camp V, resol V, cntry V, ct V, brand V, 100k 70/30'],\n",
       " [0.6447, 'ad_camp V, resol V, cntry V, ct V, brand V, 100k 30/70'],\n",
       " [0.6438, 'ad_camp v2 V, resol V, cntry V, ct V, brand V, 100k 30/70'],\n",
       " [0.6445, 'ad_camp v2 V, resol V, cntry V, ct V, brand v2 V, 100k 30/70'],\n",
       " [0.6459, 'ad_camp V, resol V, cntry V, ct V, brand v2 V, 100k 30/70'],\n",
       " [0.6312, 'ad_camp V, resol V, cntry V, ct v2 V, brand v2 V, 100k 30/70'],\n",
       " [0.6463, 'ad_camp V, resol V, cntry v 2V, ct V, brand v2 V, 100k 30/70'],\n",
       " [0.6461, 'ad_camp V, resol v2 V, cntry v2 V, ct V, brand v2 V, 100k 30/70'],\n",
       " [0.6482, 'ad_camp V, resol v2 V, cntry v2 V, ct V, brand v2 V, 100k 50/50'],\n",
       " [0.6486, 'ad_camp V, resol v2 V, cntry v2 V, ct V, brand v2 V, 100k 50/50'],\n",
       " [0.6463, 'ad_camp X, resol v2 V, cntry v2 V, ct V, brand v2 V, 100k 50/50'],\n",
       " [0.6489,\n",
       "  'ad_camp V, resol v2 V, cntry v2 V, cntry V, ct v2 V, ct V, brand v2 V, 100k 50/50'],\n",
       " [0.6605,\n",
       "  'ad_camp V, resol v2 V, cntry v2 V, cntry V, ct v2 V, ct V, brand v2 V, 200k 50/50']]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c1daf8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df5 = pd.read_csv('data/df3_200k_50n_50p.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "eff6322b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "event_action start\n",
      "event_action end\n",
      "-\n"
     ]
    }
   ],
   "source": [
    "df5 = event_action(df4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1a99f9c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200000"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b0bae07f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200000"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test = df5['event_action']\n",
    "x_test = df5.drop('event_action', axis=1)\n",
    "len(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "22f5ce51",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ad_campaign start\n",
      "ad_campaign end\n",
      "-\n",
      "day_of_week start\n",
      "day_of_week end\n",
      "-\n",
      "empties end\n",
      "empties end\n",
      "-\n",
      "resolution_func v2 start\n",
      "resolution_func v2 end\n",
      "-\n",
      "country v2  start\n",
      "country v2 end\n",
      "-\n",
      "country start\n",
      "country end\n",
      "-\n",
      "city v2  start\n",
      "city v2 end\n",
      "-\n",
      "city start\n",
      "city end\n",
      "-\n",
      "device_brand start\n",
      "device_brand end\n",
      "-\n",
      "device_brand v 2 start\n",
      "device_brand v 2 end\n",
      "-\n",
      "filter_stuff start\n",
      "filter_stuff end\n",
      "Index(['utm_source', 'utm_medium', 'utm_campaign', 'utm_adcontent',\n",
      "       'device_category', 'device_brand', 'device_screen_resolution',\n",
      "       'device_browser', 'geo_country', 'geo_city', 'camp_succ_rate',\n",
      "       'day_of_week', 'geo_country_succ_perc', 'geo_city_succ_perc',\n",
      "       'device_brand_succ_perc'],\n",
      "      dtype='object')\n",
      "-\n",
      "scale_stuff start\n",
      "utm_source\n",
      "utm_medium\n",
      "utm_campaign\n",
      "utm_adcontent\n",
      "device_category\n",
      "device_brand\n",
      "device_browser\n",
      "geo_country\n",
      "geo_city\n",
      "camp_succ_rate\n",
      "geo_country_succ_perc\n",
      "geo_city_succ_perc\n",
      "device_brand_succ_perc\n",
      "day_of_week_scaled\n",
      "device_screen_resolution_scaled\n",
      "15\n",
      "scale_stuff end\n",
      "-\n",
      "200000  test acc score -  0.443305\n"
     ]
    }
   ],
   "source": [
    "predictions = pipe_tst['model'].predict(x_test)\n",
    "\n",
    "#probs = pipeline.predict_proba(x_test)\n",
    "#scores_single_fit.append([type(model).__name__,' test roc score - ', roc_auc_score(y_test, probs[:,1])])\n",
    "#scores_single_fit.append([type(model).__name__,' test acc score - ', accuracy_score(y_test, predictions)])\n",
    "#print('test roc score - ', roc_auc_score(y_test, probs[:,1]))\n",
    "print(len(predictions),' test acc score - ', accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "cf8e2c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/pipeline_200k_50_50_tst.pkl', 'wb') as file:\n",
    "    dill.dump({\n",
    "        'model': pipeline,\n",
    "        'metadata': {\n",
    "            'name': 'sber_auto_sub_model_1',\n",
    "            'author': 'well... me, i guess:)',\n",
    "            'version': 0.00000000000000000001,\n",
    "            'type': type(pipeline.named_steps[\"classifier\"]).__name__,\n",
    "            'roc-auc': cv_scores[-1][0]\n",
    "        }\n",
    "    }, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a8e0e467",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/pipeline_200k_50_50_tst.pkl', 'rb') as file:\n",
    "    pipe_tst = dill.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4043f977",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_tst['model'] == pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "4ffa87e7",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.6375, 'ad_camp V, resol v1 V, ctry v1 V, ct v1 V, brand v1 V'],\n",
       " [0.6375, 'ad_camp X, resol v1 V, ctry v1 V, ct v1 V, brand v1 V'],\n",
       " [0.6375, 'ad_camp X, resol v1 V, ctry v1 V, ct v1 V, brand v1 V'],\n",
       " [0.6482, 'ad_camp V, resol v1 X, ctry v1 V, ct v1 V, brand v1 V'],\n",
       " [0.6504, 'ad_camp X, resol v1 X, ctry v1 X, ct v1 X, brand v1 X'],\n",
       " [0.6409, 'ad_camp X, resol v1 V, ctry v1 X, ct v1 X, brand v1 X'],\n",
       " [0.6396, 'ad_camp X, resol v2 V, ctry v1 X, ct v1 X, brand v1 X'],\n",
       " [0.6502, 'ad_camp X, resol v2 X, ctry v1 V, ct v1 X, brand v1 X'],\n",
       " [0.6503, 'ad_camp X, resol v2 X, ctry v2 V, ct v1 X, brand v1 X'],\n",
       " [0.6504, 'ad_camp X, resol v2 X, ctry v2 X, ct v1 X, brand v1 X'],\n",
       " [0.6498, 'ad_camp X, resol v2 X, ctry v2 X, ct v1 V, brand v1 X'],\n",
       " [0.6389, 'ad_camp X, resol v2 X, ctry v2 X, ct v2 V, brand v1 X'],\n",
       " [0.649, 'ad_camp X, resol v2 X, ctry v2 X, ct v2 X, brand v1 V'],\n",
       " [0.6503, 'ad_camp X, resol v2 X, ctry v2 X, ct v2 X, brand v2 V'],\n",
       " [0.6504, 'ad_camp X, resol v2 X, ctry v2 X, ct v2 X, brand v2 V'],\n",
       " [0.6504, 'ad_camp X, resol v2 X, ctry v2 X, ct v2 X, brand v2 V'],\n",
       " [0.6603, 'ad_camp V, resol V, cntry V, ct V, brand V, 200k 50/50'],\n",
       " [0.6576, 'ad_camp V, resol V, cntry V, ct V, brand V, 200k 70/30'],\n",
       " [0.6456, 'ad_camp V, resol V, cntry V, ct V, brand V, 140k 30/70'],\n",
       " [0.6469, 'ad_camp V, resol V, cntry V, ct V, brand V, 100k 50/50'],\n",
       " [0.6428, 'ad_camp V, resol V, cntry V, ct V, brand V, 100k 70/30'],\n",
       " [0.6447, 'ad_camp V, resol V, cntry V, ct V, brand V, 100k 30/70'],\n",
       " [0.6438, 'ad_camp v2 V, resol V, cntry V, ct V, brand V, 100k 30/70'],\n",
       " [0.6445, 'ad_camp v2 V, resol V, cntry V, ct V, brand v2 V, 100k 30/70'],\n",
       " [0.6459, 'ad_camp V, resol V, cntry V, ct V, brand v2 V, 100k 30/70'],\n",
       " [0.6312, 'ad_camp V, resol V, cntry V, ct v2 V, brand v2 V, 100k 30/70'],\n",
       " [0.6463, 'ad_camp V, resol V, cntry v 2V, ct V, brand v2 V, 100k 30/70'],\n",
       " [0.6461, 'ad_camp V, resol v2 V, cntry v2 V, ct V, brand v2 V, 100k 30/70'],\n",
       " [0.6482, 'ad_camp V, resol v2 V, cntry v2 V, ct V, brand v2 V, 100k 50/50'],\n",
       " [0.6486, 'ad_camp V, resol v2 V, cntry v2 V, ct V, brand v2 V, 100k 50/50'],\n",
       " [0.6463, 'ad_camp X, resol v2 V, cntry v 2V, ct V, brand v2 V, 100k 50/50'],\n",
       " [0.6489, 'ad_camp V, resol v2 V, cntry v 2V, ct V, brand v2 V']]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5f66ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a0b45c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.read_csv('data/df3_200k_50n_50p.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "84070a1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_df3 start\n",
      "sample_df3 end\n",
      "-\n"
     ]
    }
   ],
   "source": [
    "df3 = sample_df3(df3, 15000, 50, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9ece3158",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15000"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4faadedf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d72ea09f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "adc4dbb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "9bb5e819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "empties end\n",
      "empties end\n",
      "-\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "df4 = pd.read_csv('data/df3_140k_30n_70p.csv')\n",
    "df4 = empties(df4)\n",
    "feature_list = df4.columns \n",
    "\n",
    "for feature in feature_list:\n",
    "    if 'Unnamed' in feature:\n",
    "        df4 = df4.drop([feature], axis=1)\n",
    "\n",
    "for i in range(10):\n",
    "    sample_row_dict = df4.sample(n=1).to_dict(orient='records')[0]\n",
    "    with open(f'examples/example_{i}.json', 'w') as file:\n",
    "        json.dump(sample_row_dict, file)\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "3046e66a",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "session_id                  object\n",
       "hit_date                    object\n",
       "hit_time                    object\n",
       "hit_number                  object\n",
       "hit_type                    object\n",
       "hit_referer                 object\n",
       "hit_page_path               object\n",
       "event_category              object\n",
       "event_action                object\n",
       "event_label                 object\n",
       "event_value                 object\n",
       "client_id                   object\n",
       "visit_date                  object\n",
       "visit_time                  object\n",
       "visit_number                object\n",
       "utm_source                  object\n",
       "utm_medium                  object\n",
       "utm_campaign                object\n",
       "utm_adcontent               object\n",
       "utm_keyword                 object\n",
       "device_category             object\n",
       "device_os                   object\n",
       "device_brand                object\n",
       "device_model                object\n",
       "device_screen_resolution    object\n",
       "device_browser              object\n",
       "geo_country                 object\n",
       "geo_city                    object\n",
       "dtype: object"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_json('examples/example_0.json', orient='index').transpose()\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "ec9e796e",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "If using all scalar values, you must pass an index",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[119], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexamples/example_0.json\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m      2\u001b[0m     my_dict \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(file)\n\u001b[1;32m----> 3\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmy_dict\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mreset_index\n\u001b[0;32m      4\u001b[0m df\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:1764\u001b[0m, in \u001b[0;36mDataFrame.from_dict\u001b[1;34m(cls, data, orient, dtype, columns)\u001b[0m\n\u001b[0;32m   1758\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1759\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtight\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for orient parameter. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00morient\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m instead\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1761\u001b[0m     )\n\u001b[0;32m   1763\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m orient \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtight\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 1764\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1765\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1766\u001b[0m     realdata \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:664\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    658\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_mgr(\n\u001b[0;32m    659\u001b[0m         data, axes\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m: index, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: columns}, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy\n\u001b[0;32m    660\u001b[0m     )\n\u001b[0;32m    662\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m    663\u001b[0m     \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[1;32m--> 664\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[43mdict_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    665\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma\u001b[38;5;241m.\u001b[39mMaskedArray):\n\u001b[0;32m    666\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mma\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmrecords\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmrecords\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\construction.py:493\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[1;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[0;32m    489\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    490\u001b[0m         \u001b[38;5;66;03m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[0;32m    491\u001b[0m         arrays \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[1;32m--> 493\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marrays_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\construction.py:118\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[1;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verify_integrity:\n\u001b[0;32m    116\u001b[0m     \u001b[38;5;66;03m# figure out the index, if necessary\u001b[39;00m\n\u001b[0;32m    117\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 118\u001b[0m         index \u001b[38;5;241m=\u001b[39m \u001b[43m_extract_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    120\u001b[0m         index \u001b[38;5;241m=\u001b[39m ensure_index(index)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\construction.py:656\u001b[0m, in \u001b[0;36m_extract_index\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    653\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPer-column arrays must each be 1-dimensional\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    655\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m indexes \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m raw_lengths:\n\u001b[1;32m--> 656\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf using all scalar values, you must pass an index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    658\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m have_series:\n\u001b[0;32m    659\u001b[0m     index \u001b[38;5;241m=\u001b[39m union_indexes(indexes)\n",
      "\u001b[1;31mValueError\u001b[0m: If using all scalar values, you must pass an index"
     ]
    }
   ],
   "source": [
    "with open('examples/example_0.json', 'r') as file:\n",
    "    my_dict = json.load(file)\n",
    "df = pd.reaDataFrame.from_dict(my_dict).reset_index\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
